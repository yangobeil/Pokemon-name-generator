{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we give a list of all the names of the existing Pokémon so far and train the computer to generate new names that look similar. We use a recurent neural network (RNN) for this task. Since the dataset is pretty small (around 800 names) the network will be very simple. The idea for this project came from the following sources:\n",
    "- https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "- https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "- https://towardsdatascience.com/character-level-language-model-1439f5dd87fe\n",
    "- assignment 2 of week 1 of the 5th course in the deep learning specialization on Coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather and clean up the  data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that I need is the names of all the Pokémon. It must be very easy to find such a list online and download it but I found a beautiful website (https://pokeapi.co) that has an API for all sorts of information about Pokémon and I thought it would be fun to learn how to use it since I never used an API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is actually very simple to get a list of the names because there is a specific link with all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['count', 'next', 'previous', 'results'])\n"
     ]
    }
   ],
   "source": [
    "result = requests.get(\"https://pokeapi.co/api/v2/pokemon/?limit=1000\").json()\n",
    "print(result.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract just the names from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bulbasaur',\n",
       " 'ivysaur',\n",
       " 'venusaur',\n",
       " 'charmander',\n",
       " 'charmeleon',\n",
       " 'charizard',\n",
       " 'squirtle',\n",
       " 'wartortle',\n",
       " 'blastoise',\n",
       " 'caterpie']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_names = [result['results'][i]['name'] for i in range(result['count'])]\n",
    "full_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is pretty small so we can study it manually. There are many names that are repeated with some characteristic at the end, like all the alola versions of the Pokémons and the mega evolutions. Those are indicated by '-' so we will use that to get rid of the extras. This may cause a few problems with cases where the '-' is actually important. We can take care of these examples by hand:\n",
    "- mr-mime should become mr mime (number 122)\n",
    "- ho-oh should become ho oh (number 250)\n",
    "- type-null should become type null (number 772)\n",
    "- mime-jr should become mime jr (number 439)\n",
    "- should remove the tapu for the ones that contain it (numbers 785-788)\n",
    "- should transform porygon2 into porygon two (number 233)\n",
    "- porygon-z should become porygon z (number 474)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_names[121] = 'mr mime'\n",
    "full_names[249] = 'ho oh'\n",
    "full_names[771] = 'type null'\n",
    "full_names[438] = 'mime jr'\n",
    "full_names[784] = 'koko'\n",
    "full_names[785] = 'lele'\n",
    "full_names[786] = 'bulu'\n",
    "full_names[787] = 'fini'\n",
    "full_names[232] = 'porygon two'\n",
    "full_names[473] = 'porygon z'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split all the names at the '-' and keep only the first part, which if we did the cleaning correctly should be the actual name of the Pokémon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "964"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_duplicates = list(map(lambda s: s.split('-')[0], full_names))\n",
    "len(names_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got rid of the extra parts in the names so we have now a lot of repeated names, which we get rid of here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "806"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = list(set(names_duplicates))\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing that will be useful to do is add a '.' at the end of each name. This will be useful to tell the RNN that the name is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['throh.',\n",
       " 'politoed.',\n",
       " 'beartic.',\n",
       " 'ariados.',\n",
       " 'dragonair.',\n",
       " 'lapras.',\n",
       " 'chatot.',\n",
       " 'maractus.',\n",
       " 'tangela.',\n",
       " 'horsea.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = list(map(lambda s: s + '.', names))\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data cleaned up it's time to transform it into a form that the neural network will understand. The details of the model will be explained later but all we need to know is that we will input characters into the network instead of words. Each of these characters will then be converted to numbers and the conversion is done using the following dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from character to index\n",
    "char_to_index = dict( (chr(i+96), i) for i in range(1,27))\n",
    "char_to_index[' '] = 0\n",
    "char_to_index['.'] = 27\n",
    "\n",
    "# Convert from index to character\n",
    "index_to_char = dict( (i, chr(i+96)) for i in range(1,27))\n",
    "index_to_char[0] = ' '\n",
    "index_to_char[27] = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a few constants that will be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum number of characters in Pokémon names\n",
    "# this will be the number of time steps in the RNN\n",
    "max_char = len(max(names, key=len))\n",
    "\n",
    "# number of elements in the list of names, this is the number of training examples\n",
    "m = len(names)\n",
    "\n",
    "# number of potential characters, this is the length of the input of each of the RNN units\n",
    "char_dim = len(char_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we convert the list of names into a training dataset. The input X of the network is an array of size (m, max_char, char_dim). It contains a matrix for each of the m names. Each matrix contains a row for each character in the name. (Note that there are always the same number of rows and if the name doesn't have enough characters to fill the whole matrix the remaining rows contain nothing.) Each of these rows represents one character and it is encoded as a one-hot vector. This means that it is a vector of zeros with a one only in the entry that corresponds to the character that is present.\n",
    "\n",
    "The output Y is the same as the input but translated by one unit. This means that the ith character in Y is the (i+1)th one in the actual name. This means that the network predicts the character that follows a given character in a sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((m, max_char, char_dim))\n",
    "Y = np.zeros((m, max_char, char_dim))\n",
    "\n",
    "for i in range(m):\n",
    "    name = list(names[i])\n",
    "    for j in range(len(name)):\n",
    "        X[i, j, char_to_index[name[j]]] = 1\n",
    "        if j < len(name)-1:\n",
    "            Y[i, j, char_to_index[name[j+1]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that we will use is a many-to-many recurrent neural network. This is a network that contains a given number of 'time' steps that each act with the same weights on the individual inputs and are all connected. Each time step takes in one input (in this case one character) and outputs a one-hot vector that represents the probabilities for the input of the next time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of interest here we only consider one layer of recurrence, which we take to be LSTM with 128 units. We return the output of this layer and use it into a fully connected dense layer that converts the result of the LSTM layer into a vector of size char_dim using a softmax activation. We use categorical cross entropy as a cost function because of the softmax result and use Adam optimization. There is not really any useful metric to judge if the model does good so we will mostly just look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(max_char, char_dim), return_sequences=True))\n",
    "model.add(Dense(char_dim, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this model will be trained we will use it to create new Pokémon names. This is achieved using the following function. The idea is to input empty characters to the trained network and use the output of the first time step as a probability distribution for the first letter of the name. We then use this distribution to decide randomly the first character, record it and update the input to pass this character as an input for the second time step. This is continued for the following time steps to create a name.\n",
    "\n",
    "This is where using a '.' at the end of each name becomes important, because we stop the procedure once we get a '.' as an output, meaning that the generated name is done. Also if we reach the length of the largest name in the training set we put a '.' and end the procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_name(model):\n",
    "    name = []\n",
    "    x = np.zeros((1, max_char, char_dim))\n",
    "    end = False\n",
    "    i = 0\n",
    "    \n",
    "    while end==False:\n",
    "        probs = list(model.predict(x)[0,i])\n",
    "        probs = probs / np.sum(probs)\n",
    "        index = np.random.choice(range(char_dim), p=probs)\n",
    "        if i == max_char-2:\n",
    "            character = '.'\n",
    "            end = True\n",
    "        else:\n",
    "            character = index_to_char[index]\n",
    "        name.append(character)\n",
    "        x[0, i+1, index] = 1\n",
    "        i += 1\n",
    "        if character == '.':\n",
    "            end = True\n",
    "    \n",
    "    print(''.join(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to use this function during the training to monitor how the generated names get better. To this end we create a function that will be given to the model when we fit it. We basically run the previous function a few times every 50 epochs and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name_loop(epoch, _):\n",
    "    if epoch % 25 == 0:\n",
    "        \n",
    "        print('Names generated after epoch %d:' % epoch)\n",
    "\n",
    "        for i in range(3):\n",
    "            make_name(model)\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This converts the function to be able to use it in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_generator = LambdaCallback(on_epoch_end = generate_name_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model with the function and look at the results. It is clear that the names make more and more sense as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names generated after epoch 0:\n",
      "dxjaemprwpk.\n",
      "zykhv.\n",
      "uzvlzwbvisa.\n",
      "\n",
      "Names generated after epoch 25:\n",
      ".\n",
      "nponre.\n",
      "tiagy.\n",
      "\n",
      "Names generated after epoch 50:\n",
      "onteel.\n",
      "ouf.\n",
      "memowskros.\n",
      "\n",
      "Names generated after epoch 75:\n",
      "sarlite.\n",
      " ichhile.\n",
      ".\n",
      "\n",
      "Names generated after epoch 100:\n",
      "heppodosn.\n",
      "regentes.\n",
      "hiternotr.\n",
      "\n",
      "Names generated after epoch 125:\n",
      "ugerli.\n",
      "yunof.\n",
      "mhanurs.\n",
      "\n",
      "Names generated after epoch 150:\n",
      "uflon.\n",
      "howedile.\n",
      "labet.\n",
      "\n",
      "Names generated after epoch 175:\n",
      "repig.\n",
      "andurat.\n",
      "randrul.\n",
      "\n",
      "Names generated after epoch 200:\n",
      "actreo.\n",
      "imosease.\n",
      "idriouno.\n",
      "\n",
      "Names generated after epoch 225:\n",
      "ilicinioaa.\n",
      "arbok.\n",
      "umphoos.\n",
      "\n",
      "Names generated after epoch 250:\n",
      "elekrdsss.\n",
      "nislea.\n",
      "hoon.\n",
      "\n",
      "Names generated after epoch 275:\n",
      "urcono.\n",
      "iggyy.\n",
      "louk.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x239431999e8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, batch_size=64, epochs=300, callbacks=[name_generator], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can no use the final trained model to generate names as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " om.\n",
      "yanmog.\n",
      " oto.\n",
      "entor.\n",
      "weinole.\n",
      " om.\n",
      "oosgiss.\n",
      "angorstr.\n",
      "oscalish.\n",
      "utterg.\n",
      "pine.\n",
      "lickio.\n",
      "incono.\n",
      "enege.\n",
      "cameruee.\n",
      "inccinon.\n",
      "unteri.\n",
      "weidoe.\n",
      "areacita.\n",
      "rielu.\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    make_name(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be possible to make the network better by changing the hyperparameters (number of units in LSTM layer, parameters of Adam optimization, adding extra LSTM layer) but I am actually pretty satisfied with the results that I get so I will leave it at that. One thing to notice is that there are some cases where the model seems to have overfitted and we recover known names or names very close to known ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
